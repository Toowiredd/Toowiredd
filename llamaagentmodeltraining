{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104433,"sourceType":"modelInstanceVersion","modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nfrom datetime import datetime\nimport transformers\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset\n\n# Set up model path and device\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b/2\"\ndevice = \"cuda\"\n\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load ADHD dataset (you'll need to create or obtain this)\ntrain_dataset = load_dataset('your_adhd_dataset', split='train')\neval_dataset = load_dataset('your_adhd_dataset', split='validation')\n\ndef tokenize(prompt):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ndef generate_and_tokenize_prompt(data_point):\n    full_prompt = f\"\"\"Given an ADHD-related task or situation, provide a helpful strategy or response.\n\n### Task/Situation:\n{data_point[\"task\"]}\n\n### Strategy/Response:\n{data_point[\"strategy\"]}\n\"\"\"\n    return tokenize(full_prompt)\n\ntokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\ntokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n\n# Set up LoRA configuration\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)\n\n# Prepare model for training\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, config)\n\n# Set up trainer\nproject = \"adhd-management-finetune\"\nbase_model_name = \"llama3.1\"\nrun_name = base_model_name + \"-\" + project\noutput_dir = \"./\" + run_name\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=transformers.TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=5,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        max_steps=1000,  # Increased for better performance\n        learning_rate=2.5e-5,\n        logging_steps=50,\n        fp16=True,\n        optim=\"paged_adamw_8bit\",\n        logging_dir=\"./logs\",\n        save_strategy=\"steps\",\n        save_steps=100,\n        eval_steps=100,\n        do_eval=True,\n        report_to='none',\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Train the model\nmodel.config.use_cache = False\ntrainer.train()\n\n# Load and evaluate the fine-tuned model\nft_model = PeftModel.from_pretrained(model, f\"{output_dir}/checkpoint-1000\")\nft_model.eval()\n\n# Example evaluation\neval_prompt = \"\"\"Given an ADHD-related task or situation, provide a helpful strategy or response.\n\n### Task/Situation:\nI often forget important deadlines for my work projects.\n\n### Strategy/Response:\n\"\"\"\n\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    generated_text = tokenizer.decode(\n        ft_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0],\n        skip_special_tokens=True\n    )\n    print(generated_text)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}